{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f26ffa-36b5-4566-83d5-a2700eac6c92",
   "metadata": {},
   "source": [
    "# <center>DeepLearning: Assignment 03</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61af99e-6161-4c70-8476-58f9dda0bee4",
   "metadata": {},
   "source": [
    "# Question 01\n",
    "\n",
    "Is it OK to initialize all the weights to the same value as long as that value is selected\n",
    "randomly using He initialization?\n",
    "\n",
    "\n",
    "## <span style='color:blue'>Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4526fa-ef48-4b1c-abc6-18b747f4faa4",
   "metadata": {},
   "source": [
    "No, initializing all the weights to the same value, even if chosen randomly using He initialization, is not recommended. He initialization is designed to address the issue of vanishing gradients during training by adjusting the scale of weights based on the number of input units. However, initializing all weights to the same value would negate the purpose of He initialization.\n",
    "\n",
    "In He initialization, weights are initialized with random values drawn from a normal distribution with mean 0 and variance 2 divided by the number of input units in the neuron's layer. This helps prevent gradients from becoming too small during backpropagation, enabling more stable and efficient training.\n",
    "\n",
    "If all weights are initialized to the same value, there will be no diversity in the initial weights, and the network may face symmetry issues, hindering the learning process. It's crucial to allow diversity in the initial weights, even when using techniques like He initialization, to enable the network to explore different pathways during training and converge to an optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e12a0-abec-45af-a487-794608e931d2",
   "metadata": {},
   "source": [
    "# Question 02\n",
    "\n",
    "Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "## <span style='color:blue'>Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1d9d9-c91d-4fb7-98f3-8a975055e4ca",
   "metadata": {},
   "source": [
    "Initializing bias terms to 0 is a common practice and generally acceptable in neural network initialization. Unlike weight initialization, biases being set to 0 doesn't lead to issues related to symmetry or vanishing gradients during training. During the training process, the network adjusts both weights and biases based on the gradients calculated during backpropagation. \n",
    "\n",
    "However, some variations in initializing biases might be beneficial in certain scenarios. For example, in batch normalization, biases are initialized to non-zero values to prevent scaling issues during training. Additionally, some specific activation functions or network architectures might respond differently to different bias initialization strategies.\n",
    "\n",
    "In most cases, initializing biases to 0 is a reasonable default, and modern deep learning frameworks often handle bias initialization automatically based on the network's structure and the chosen activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa728be-aba6-4f19-8ad7-52b9ccc6ff56",
   "metadata": {},
   "source": [
    "# Question 03\n",
    "\n",
    "Name three advantages of the SELU activation function over ReLU.\n",
    "\n",
    "## <span style='color:blue'>Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a263f5-1fdc-46e6-83c4-13e17170e30b",
   "metadata": {},
   "source": [
    "The Scaled Exponential Linear Unit (SELU) activation function offers several advantages over the Rectified Linear Unit (ReLU) activation function:\n",
    "\n",
    "1. **Non-Zero Mean Output:**\n",
    "   - SELU has a non-zero mean output, which helps address the vanishing gradients problem. Unlike ReLU, which can output zero for negative inputs, SELU ensures a non-zero mean, promoting stable gradients during training.\n",
    "\n",
    "2. **Self-Normalizing Properties:**\n",
    "   - SELU has self-normalizing properties, meaning it can stabilize the activations of neurons during training. Neural networks with many layers and SELU activations tend to converge faster and generalize better, especially in deep architectures, without the need for elaborate normalization techniques like Batch Normalization.\n",
    "\n",
    "3. **Preservation of Network Weights:**\n",
    "   - SELU activations tend to preserve the magnitude of network weights during training. This means the weights don't explode to very large values or diminish to very small values, leading to more stable and effective learning.\n",
    "\n",
    "These advantages make SELU a favorable choice in deep neural networks, particularly when dealing with architectures where stable gradients and consistent weight updates are crucial for efficient and effective learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a1116-7009-4e53-ae6e-247e3c458a18",
   "metadata": {},
   "source": [
    "# Question 04\n",
    "\n",
    "In which cases would you want to use each of the following activation functions: SELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "## <span style='color:blue'>Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad085697-d0e3-459e-bd40-9f05484e8879",
   "metadata": {},
   "source": [
    "1. **SELU (Scaled Exponential Linear Unit):**\n",
    "   - **When to Use:** Use SELU when building deep neural networks, especially deep feedforward networks (MLPs).\n",
    "   - **Advantages:** SELU can mitigate vanishing gradients and stabilize training in deep architectures, leading to faster convergence and better generalization.\n",
    "   - **Note:** Ensure the input features are standardized (zero mean and unit variance) for SELU to exhibit its self-normalizing properties effectively.\n",
    "\n",
    "2. **Leaky ReLU and Its Variants (e.g., Parametric ReLU, Randomized Leaky ReLU):**\n",
    "   - **When to Use:** Use Leaky ReLU and its variants when dealing with dead neurons (neurons always output 0) in traditional ReLU activations.\n",
    "   - **Advantages:** Leaky ReLU variants address the dying ReLU problem by allowing small negative slopes for negative inputs, preventing neurons from becoming inactive during training.\n",
    "  \n",
    "3. **ReLU (Rectified Linear Unit):**\n",
    "   - **When to Use:** Use ReLU as a default choice for most hidden layers in deep neural networks.\n",
    "   - **Advantages:** ReLU is computationally efficient and helps mitigate vanishing gradients for positive inputs, promoting faster training for deep networks.\n",
    "   - **Note:** Watch out for dying ReLU problem (neurons always output 0 for negative inputs). If this occurs, consider using Leaky ReLU or its variants.\n",
    "\n",
    "4. **Tanh (Hyperbolic Tangent):**\n",
    "   - **When to Use:** Use tanh for hidden layers when the data is centered around 0 (e.g., mean-shifted to zero) or when the output range needs to be between -1 and 1.\n",
    "   - **Advantages:** Tanh squashes inputs to the range [-1, 1], making it zero-centered, which can help learning in certain scenarios, especially in the hidden layers of neural networks.\n",
    "  \n",
    "5. **Logistic (Sigmoid):**\n",
    "   - **When to Use:** Use logistic (sigmoid) for binary classification problems in the output layer.\n",
    "   - **Advantages:** Sigmoid function maps inputs to the range [0, 1], making it suitable for binary classification tasks where the output needs to represent probabilities.\n",
    "   - **Note:** Avoid using sigmoid in hidden layers of deep networks due to vanishing gradients, unless specifically required for a certain design reason.\n",
    "  \n",
    "6. **Softmax:**\n",
    "   - **When to Use:** Use softmax in the output layer for multi-class classification problems.\n",
    "   - **Advantages:** Softmax function converts raw scores (logits) into probabilities, allowing the model to predict multiple classes mutually exclusively.\n",
    "   - **Note:** Softmax is essential for multi-class problems, ensuring that the class probabilities sum up to 1, making it suitable for classification tasks with more than two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c495f-be41-47c7-be12-b1f23575ad30",
   "metadata": {},
   "source": [
    "# Question 05\n",
    "\n",
    "What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
    "when using an SGD optimizer?\n",
    "\n",
    "\n",
    "## <span style='color:blue'>Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388bf17-7dd1-4b9e-8153-1913ac1df35f",
   "metadata": {},
   "source": [
    "Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) in Stochastic Gradient Descent (SGD) can lead to several issues:\n",
    "\n",
    "1. **Reduced Learning Rate Effectiveness:** Momentum enhances the effective learning rate, allowing the optimizer to continue moving in the previous direction with a certain velocity. If momentum is extremely close to 1, it essentially retains almost all the previous velocity, making the learning rate's impact negligible. This can slow down the learning process, as the updates become very small.\n",
    "\n",
    "2. **Overshooting and Instability:** With extremely high momentum, the optimizer might overshoot the optimal point. It can cause the algorithm to oscillate around the minimum without converging. The momentum can become so dominant that the optimizer might miss the optimal solution completely.\n",
    "\n",
    "3. **Dampened Responsiveness:** High momentum makes the optimizer less responsive to local changes in the loss landscape. While momentum is supposed to help the optimizer escape local minima, excessively high momentum can prevent the optimizer from exploring new regions of the parameter space, leading to suboptimal solutions.\n",
    "\n",
    "4. **Difficulty in Convergence Analysis:** Extremely high momentum can make it difficult to analyze the convergence behavior of the optimization algorithm. It might converge, diverge, or oscillate unpredictably, making it challenging to determine the convergence criteria and stopping conditions.\n",
    "\n",
    "5. **Difficulty in Escaping Local Minima:** While momentum helps escape local minima, setting it too close to 1 might cause the optimizer to overshoot the global minimum and get stuck in a new local minimum, especially in non-convex optimization problems.\n",
    "\n",
    "To avoid these issues, it's crucial to carefully choose the momentum hyperparameter based on the specific problem and dataset. It's often beneficial to start with moderate values (e.g., 0.9) and tune the hyperparameters through experimentation and validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a862b1d5-9844-4bcc-9205-346423b02127",
   "metadata": {},
   "source": [
    "# Question 06\n",
    "\n",
    "Name three ways you can produce a sparse model.\n",
    "\n",
    "## <span style='color:blue'>Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52e357-62c7-4c41-8421-fd6666ad000d",
   "metadata": {},
   "source": [
    "1. **L1 Regularization (Lasso):**\n",
    "   - Introducing an L1 penalty in the loss function encourages sparsity by pushing irrelevant or less relevant features' weights towards zero. Features with zero weights are effectively pruned, creating a sparse model.\n",
    "\n",
    "2. **Feature Selection Techniques:**\n",
    "   - Utilize feature selection methods like Mutual Information, Recursive Feature Elimination, or Tree-based feature importance to identify and keep only the most informative features, discarding irrelevant or redundant ones, leading to a sparser representation.\n",
    "\n",
    "3. **Dropout in Neural Networks:**\n",
    "   - In neural networks, dropout is a regularization technique where random neurons are temporarily removed during training. This randomness encourages the network to rely on multiple pathways, promoting a sparse network by preventing over-reliance on specific neurons or features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d72a3-b5a7-480e-bbfe-476546c6d3e1",
   "metadata": {},
   "source": [
    "# Question 07\n",
    "\n",
    "Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
    "new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0e0e9-112c-4710-90f0-db6580369b2c",
   "metadata": {},
   "source": [
    "1. **Dropout and Training Speed:**\n",
    "   - Dropout can slightly slow down the training process because, during each training iteration, a fraction of neurons is randomly dropped out, requiring additional computations. However, this slowdown is usually not significant, and dropout is still widely used in practice due to its regularization benefits.\n",
    "\n",
    "2. **Dropout and Inference Speed:**\n",
    "   - Dropout does not slow down inference (making predictions on new instances). During inference, dropout is typically turned off, and the model operates with all neurons active. Therefore, the prediction speed is not affected by dropout regularization.\n",
    "\n",
    "3. **MC Dropout (Monte Carlo Dropout):**\n",
    "   - MC Dropout involves performing multiple forward passes with dropout enabled during inference and averaging the predictions. While this technique introduces additional computations, it provides uncertainty estimates along with predictions, making it valuable for tasks like uncertainty quantification or Bayesian neural networks.\n",
    "   - MC Dropout can significantly slow down inference, especially if a large number of forward passes are required for accurate uncertainty estimation. The trade-off between prediction accuracy and computational cost needs to be considered when using MC Dropout in real-time applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d8d8d-5bdf-41b4-8cfb-7d061758b1d2",
   "metadata": {},
   "source": [
    "# Question 08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361cc288-de17-420f-993e-3ebb832acfe1",
   "metadata": {},
   "source": [
    "**a) Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.**\n",
    "\n",
    "## <span style='color:blue'>Answer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70fe3d-1823-4f98-af4d-b2c5f1533449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.activations import elu\n",
    "\n",
    "# Build the DNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer (assuming input shape is known, adjust input_shape accordingly)\n",
    "model.add(Dense(units=100, activation=elu, kernel_initializer=HeNormal(), input_shape=(input_shape,)))\n",
    "\n",
    "# 20 hidden layers with 100 neurons each\n",
    "for _ in range(20):\n",
    "    model.add(Dense(units=100, activation=elu, kernel_initializer=HeNormal()))\n",
    "\n",
    "# Output layer with appropriate number of units/neurons for your task (e.g., classification)\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1c47d-de2a-49de-939d-b6260e4d92d9",
   "metadata": {},
   "source": [
    "**b) Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
    "dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
    "composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
    "testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
    "Remember to search for the right learning rate each time you change the model’s\n",
    "architecture or hyperparameters.**\n",
    "\n",
    "\n",
    "## <span style='color:blue'>Answer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ccb3d-c46e-4c62-9c1b-ec9dbace1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.activations import elu\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to be between 0 and 1\n",
    "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)  # One-hot encode labels\n",
    "\n",
    "# Build the DNN model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(32, 32, 3)))  # Flatten input images\n",
    "for _ in range(20):\n",
    "    model.add(Dense(100, activation=elu, kernel_initializer=HeNormal()))  # 20 hidden layers with 100 neurons each\n",
    "model.add(Dense(10, activation='softmax'))  # Output layer with 10 neurons for 10 classes\n",
    "\n",
    "# Compile the model with Nadam optimizer and categorical crossentropy loss\n",
    "model.compile(optimizer=Nadam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=32, \n",
    "                    validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f942307-ede0-4a21-94c2-39ab86791ae6",
   "metadata": {},
   "source": [
    "**c) Now try adding Batch Normalization and compare the learning curves: Is it\n",
    "converging faster than before? Does it produce a better model? How does it affect\n",
    "training speed?**\n",
    "\n",
    "## <span style='color:blue'>Answer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac60a950-daaa-4bf6-8785-ad8fe7cbcfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.activations import elu\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to be between 0 and 1\n",
    "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)  # One-hot encode labels\n",
    "\n",
    "# Build the DNN model with Batch Normalization\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(32, 32, 3)))  # Flatten input images\n",
    "for _ in range(20):\n",
    "    model.add(Dense(100, activation=None, kernel_initializer=HeNormal()))  # 20 hidden layers with 100 neurons each\n",
    "    model.add(BatchNormalization())  # Batch Normalization layer\n",
    "    model.add(Activation(elu))  # ELU activation function\n",
    "model.add(Dense(10, activation='softmax'))  # Output layer with 10 neurons for 10 classes\n",
    "\n",
    "# Compile the model with Nadam optimizer and categorical crossentropy loss\n",
    "model.compile(optimizer=Nadam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history_with_batch_norm = model.fit(x_train, y_train, epochs=100, batch_size=32, \n",
    "                                    validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy (with Batch Normalization): {test_accuracy:.2f}\")\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy (Without Batch Norm)')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy (Without Batch Norm)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Learning Curves without Batch Normalization')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_with_batch_norm.history['accuracy'], label='Training Accuracy (With Batch Norm)')\n",
    "plt.plot(history_with_batch_norm.history['val_accuracy'], label='Validation Accuracy (With Batch Norm)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Learning Curves with Batch Normalization')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0679f2b8-ef3a-41ee-b7e7-9da181ebb987",
   "metadata": {},
   "source": [
    "**d) Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
    "to ensure the network self-normalizes (i.e., standardize the input features, use\n",
    "LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
    "layers, etc.).**\n",
    "\n",
    "## <span style='color:blue'>Answer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a674a7-0d40-4bf3-9b4b-23399c75ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import LeCunNormal\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Preprocess the data (standardize input features)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Build the DNN model with SELU activation and LeCun initialization\n",
    "model = Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(32, 32, 3)))  # Flatten input images\n",
    "for _ in range(20):\n",
    "    model.add(Dense(100, activation='selu', kernel_initializer=LeCunNormal()))  # 20 hidden layers with SELU activation\n",
    "model.add(Dense(10, activation='softmax'))  # Output layer with 10 neurons for 10 classes\n",
    "\n",
    "# Compile the model with Nadam optimizer and categorical crossentropy loss\n",
    "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history_with_selu = model.fit(x_train, y_train, epochs=100, batch_size=32, \n",
    "                               validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy (with SELU activation): {test_accuracy:.2f}\")\n",
    "\n",
    "# Plot learning curves\n",
    "plt.plot(history_with_selu.history['accuracy'], label='Training Accuracy (SELU)')\n",
    "plt.plot(history_with_selu.history['val_accuracy'], label='Validation Accuracy (SELU)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Learning Curves with SELU Activation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87a79d-222f-408a-919c-7da284975d81",
   "metadata": {},
   "source": [
    "**e) Try regularizing the model with alpha dropout. Then, without retraining your model,\n",
    "see if you can achieve better accuracy using MC Dropout.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf92d993-95e6-4575-9ddc-3ec19fc0a93e",
   "metadata": {},
   "source": [
    "1. **Regularize the Model with Alpha Dropout:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fdee3-2898-4e93-8c75-e5a75bdde90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, AlphaDropout\n",
    "from tensorflow.keras.initializers import LeCunNormal\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Preprocess the data (standardize input features)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Build the DNN model with SELU activation, LeCun initialization, and Alpha Dropout regularization\n",
    "model = Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(32, 32, 3)))  # Flatten input images\n",
    "for _ in range(20):\n",
    "    model.add(Dense(100, activation='selu', kernel_initializer=LeCunNormal()))  # 20 hidden layers with SELU activation\n",
    "    model.add(AlphaDropout(rate=0.1))  # Alpha Dropout regularization with dropout rate 0.1\n",
    "model.add(Dense(10, activation='softmax'))  # Output layer with 10 neurons for 10 classes\n",
    "\n",
    "# Compile the model with Nadam optimizer and categorical crossentropy loss\n",
    "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history_with_alpha_dropout = model.fit(x_train, y_train, epochs=100, batch_size=32, \n",
    "                                       validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy (with Alpha Dropout): {test_accuracy:.2f}\")\n",
    "\n",
    "# Plot learning curves\n",
    "plt.plot(history_with_alpha_dropout.history['accuracy'], label='Training Accuracy (Alpha Dropout)')\n",
    "plt.plot(history_with_alpha_dropout.history['val_accuracy'], label='Validation Accuracy (Alpha Dropout)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Learning Curves with Alpha Dropout')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b40ed-ba31-485d-a8b4-f354465877fc",
   "metadata": {},
   "source": [
    "2. **Apply MC Dropout for Improved Accuracy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80cc667-c74c-4c01-a9a6-1d93a31105ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of Monte Carlo samples (forward passes) for MC Dropout\n",
    "num_mc_samples = 100\n",
    "\n",
    "# Predict using MC Dropout (perform multiple forward passes and average predictions)\n",
    "predictions = np.zeros((len(x_test), 10))\n",
    "for _ in range(num_mc_samples):\n",
    "    predictions += model.predict(x_test, batch_size=32)\n",
    "predictions /= num_mc_samples\n",
    "\n",
    "# Calculate accuracy based on MC Dropout predictions\n",
    "mc_dropout_accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "print(f\"MC Dropout Accuracy: {mc_dropout_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae9a89-a7b9-4b0b-8ba9-2f5d6679d31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
