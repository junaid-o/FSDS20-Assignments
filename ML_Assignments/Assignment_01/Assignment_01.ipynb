{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118935c0-4171-4247-b8cc-78ef9726edf0",
   "metadata": {},
   "source": [
    "# <center>MachineLearning: Assignment_01</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38398678-5c63-42c1-9f19-e2386d6f63b6",
   "metadata": {},
   "source": [
    "### Question 01:\n",
    "What does one mean by the term &quot;machine learning&quot;?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa978c-00ee-4753-b2d9-0a9ed749abbd",
   "metadata": {},
   "source": [
    "Machine learning refers to a branch of artificial intelligence (AI) that focuses on developing algorithms and models capable of learning and making predictions or decisions without explicit programming. It involves the study of computational methods and systems that can learn from data and improve their performance through experience.\r\n",
    "\r\n",
    "In machine learning, algorithms are designed to analyze data, identify patterns, and derive meaningful insights. By training these algorithms on a large dataset, they can learn from the data and recognize relationships or patterns. Once trained, these models can be used to make predictions or decisions on new, unseen datnsights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f909a00-dd4c-4b9c-a9f4-c445d21c47d3",
   "metadata": {},
   "source": [
    "### Question 02:\n",
    "Can you think of 4 distinct types of issues where it shines?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78bbac-5be2-44fc-b884-6e6b2232a958",
   "metadata": {},
   "source": [
    "1. Image and Object Recognition: Machine learning excels in tasks such as image classification, object detection, and facial recognition. It can accurately identify and categorize objects within images, enabling applications like self-driving cars, surveillance systems, and medical image analysis.\n",
    "\n",
    "2. Natural Language Processing: Machine learning plays a crucial role in natural language processing tasks such as sentiment analysis, language translation, chatbots, and speech recognition. It enables machines to understand, generate, and respond to human language, facilitating advancements in virtual assistants, customer support systems, and language-based applications.\n",
    "\n",
    "3. Recommendation Systems: Machine learning algorithms are extensively used in recommendation systems, which provide personalized suggestions to users based on their preferences and behaviors. Examples include movie recommendations on streaming platforms, product recommendations on e-commerce websites, and content recommendations on social media platforms.\n",
    "\n",
    "4. Predictive Analytics: Machine learning enables predictive analytics by analyzing historical data and identifying patterns to make predictions about future outcomes. It is used in various domains, including finance, healthcare, marketing, and manufacturing, to forecast customer behavior, predict disease progression, detect anomalies, optimize operations, and make data-driven decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924c2b4-3c77-40e1-882a-e53b99da949a",
   "metadata": {},
   "source": [
    "### Question 03:\n",
    "What is a labeled training set, and how does it work?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f67bcb-53b8-4585-8a6d-74098178e197",
   "metadata": {},
   "source": [
    "A labeled training set is a dataset used to train a machine learning model, where each data point is paired with a corresponding label or output value. The labels provide the desired or correct outputs for the given inputs. During training, the machine learning algorithm analyzes the labeled training set to learn patterns and correlations between input features and labels. It adjusts its internal parameters or weights to minimize the discrepancy between predicted outputs and true labels. The trained model can then make predictions or classifications on new data based on its learned patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec7db2-a489-4530-bf53-4eb794ea852f",
   "metadata": {},
   "source": [
    "### Question 04:\n",
    "What are the two most important tasks that are supervised?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34512864-47c2-453b-a5fc-630d45d51834",
   "metadata": {},
   "source": [
    "The two most important tasks in supervised learning are:\n",
    "\n",
    "1. Classification: Classification is a supervised learning task where the goal is to assign input data points to pre-defined classes or categories. In classification, the labeled training set consists of input samples along with their corresponding class labels. The objective is to train a model that can accurately classify new, unseen data into the correct classes. Examples of classification tasks include email spam detection, sentiment analysis, disease diagnosis, and image/object recognition.\n",
    "\n",
    "2. Regression: Regression is another important supervised learning task that involves predicting a continuous output or numerical value based on input features. In regression, the labeled training set contains input data points along with their corresponding continuous target values. The goal is to train a model that can learn the relationships between the input features and the target values, allowing it to predict the target value for new input data. Examples of regression tasks include predicting housing prices, stock market forecasting, and demand forecasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216080a-862d-4691-972c-73da9a0e9de2",
   "metadata": {},
   "source": [
    "### Question 05:\n",
    "Can you think of four examples of unsupervised tasks?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a708f6c6-e643-4bb9-9888-f74233787c4a",
   "metadata": {},
   "source": [
    "1. **Clustering**: Clustering is an unsupervised learning task where the goal is to group similar data points together based on their inherent patterns or similarities. The algorithm analyzes the input data without any pre-defined class labels and identifies clusters or groups of data points that share similar characteristics. It helps in discovering hidden structures or segments within the data. Clustering is used in various applications such as customer segmentation, document clustering, image segmentation, and anomaly detection.\n",
    "\n",
    "2. **Dimensionality Reduction**: Dimensionality reduction refers to the task of reducing the number of input features or variables while preserving the essential information in the data. It is useful when dealing with high-dimensional data, as it can simplify the data representation, remove noise, and improve computational efficiency. Techniques like Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) are commonly used for dimensionality reduction. It finds applications in visualizations, feature selection, and preprocessing for other machine learning tasks.\n",
    "\n",
    "3. **Association Rule Mining**: Association rule mining involves discovering interesting relationships or associations among items in large datasets. It aims to identify frequently occurring patterns, co-occurrences, or correlations between items or events. This task is commonly used in market basket analysis, where the goal is to find associations between items frequently purchased together. It can help in understanding customer behavior, product recommendations, and optimizing business strategies.\n",
    "\n",
    "4. **Anomaly Detection**: Anomaly detection, also known as outlier detection, focuses on identifying rare or abnormal instances in a dataset. The goal is to distinguish data points that significantly deviate from the expected or normal behavior. Anomaly detection is used in various domains such as fraud detection, network intrusion detection, system monitoring, and preventive maintenance. Unsupervised anomaly detection techniques learn patterns from the majority of the data and flag instances that do not conform to those patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba396ce-6f54-4b67-bcfa-bde0dee17c59",
   "metadata": {},
   "source": [
    "### Question 06:\n",
    "State the machine learning model that would be best to make a robot walk through various\n",
    "unfamiliar terrains?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f2cd2-670f-4f55-bffa-72dd761bfa34",
   "metadata": {},
   "source": [
    "The machine learning model that would be best suited for making a robot walk through various unfamiliar terrains is a Reinforcement Learning model, specifically a policy-based reinforcement learning algorithm such as Proximal Policy Optimization (PPO).\n",
    "\n",
    "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions or take actions in an environment to maximize a cumulative reward signal. In the case of the robot walking through unfamiliar terrains, RL can enable the robot to learn how to navigate and adapt its walking behavior based on the feedback received from the environment.\n",
    "\n",
    "PPO is a popular policy-based reinforcement learning algorithm that optimizes a policy, which is a mapping from states to actions. It is well-suited for continuous control tasks like robot locomotion. PPO iteratively collects data by interacting with the environment, uses this data to update the policy, and gradually improves the robot's walking behavior.\n",
    "\n",
    "By training a robot with PPO, it can learn to explore and adapt its walking strategy to different terrains, adjust its movements based on feedback signals (such as balance or stability), and optimize its locomotion policy over time to maximize the walking performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df559a99-ecc7-4a6a-b168-c412e5547f29",
   "metadata": {},
   "source": [
    "### Question 07:\n",
    "Which algorithm will you use to divide your customers into different groups?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820e62e-9cc3-4d20-b58f-0e8b1f7f6b4a",
   "metadata": {},
   "source": [
    "To divide customers into different groups, a commonly used algorithm is K-means clustering. K-means clustering is an unsupervised learning algorithm that aims to partition a dataset into distinct clusters based on the similarity of data points.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1. Define the desired number of clusters (K) to divide the customers into.\n",
    "2. Initialize K cluster centroids randomly within the data space.\n",
    "3. Assign each customer to the nearest centroid based on a distance metric (usually Euclidean distance).\n",
    "4. Recalculate the centroids by taking the mean of all the data points assigned to each cluster.\n",
    "5. Repeat steps 3 and 4 until the centroids stabilize or a maximum number of iterations is reached.\n",
    "6. The final centroids represent the centers of the different customer groups.\n",
    "\n",
    "By using K-means clustering, customers can be grouped based on shared characteristics or behaviors, allowing businesses to gain insights into their customer base and tailor their strategies accordingly. These groups can be useful for targeted marketing campaigns, customer segmentation, personalized recommendations, and understanding customer preferences.\n",
    "\n",
    "It's worth noting that K-means clustering assumes that the number of clusters (K) is known in advance. If the optimal number of clusters is unknown, other techniques like hierarchical clustering or density-based clustering algorithms such as DBSCAN can be considered. Additionally, the choice of clustering algorithm may depend on the specific characteristics and requirements of the customer data and the goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed9e97-f3f4-4964-a1d6-d1cfbe70c932",
   "metadata": {},
   "source": [
    "### Question 08:\n",
    "Will you consider the problem of spam detection to be a supervised or unsupervised learning\n",
    "problem?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8c890-9c96-4893-bfe3-bdfd7db3a3ef",
   "metadata": {},
   "source": [
    "The problem of spam detection is typically considered a supervised learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f12294-7b86-4307-88ec-5512438b6c03",
   "metadata": {},
   "source": [
    "### Question 09:\n",
    "What is the concept of an online learning system?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d4aec-888e-4e75-9b0a-22b62b8cdcf9",
   "metadata": {},
   "source": [
    "### Question 10:\n",
    "What is out-of-core learning, and how does it differ from core learning?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5a6d6-2011-4565-bd34-78ed2772c4bc",
   "metadata": {},
   "source": [
    "**Out-of-core learning**, also known as online learning or streaming learning, is a technique used in machine learning to handle large datasets that cannot fit into memory. It is specifically designed for situations where the data is too large to be processed in one go, and it needs to be processed in smaller, manageable portions.\n",
    "\n",
    "In out-of-core learning, the algorithm processes data in mini-batches or chunks, reading a subset of the data from disk or other storage devices, performing computations on that subset, and then updating the model parameters. This process is repeated iteratively for each mini-batch until the entire dataset has been processed. The model's parameters are updated incrementally as new data arrives, which allows the model to adapt and learn from new examples continuously.\n",
    "\n",
    "Out-of-core learning is particularly useful when working with large-scale datasets or when the data is continuously streaming or arriving in real-time. It enables the training process to be performed efficiently and effectively without requiring the entire dataset to be loaded into memory at once.\n",
    "\n",
    "On the other hand, **\"core learning\"** is not a commonly used term in machine learning. It could be a reference to \"in-core learning,\" which simply means traditional machine learning techniques where the entire dataset is loaded into memory before training. In-core learning assumes that the entire dataset can fit into memory, making it easier to perform computations and optimize the learning process. In-core learning is suitable for smaller datasets where memory constraints are not an issue.\n",
    "\n",
    "Overall, out-of-core learning handles large datasets that do not fit into memory by processing data in smaller chunks or mini-batches, while core learning (in-core learning) refers to traditional learning methods where the entire dataset is loaded into memory for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64e228-9f4a-4ed9-a6d6-dcfcc00fef76",
   "metadata": {},
   "source": [
    "### Question 11:\n",
    "What kind of learning algorithm makes predictions using a similarity measure?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e741122-a151-48c7-ac1c-4099b34dbc89",
   "metadata": {},
   "source": [
    "The learning algorithm that makes predictions using a similarity measure is called **Instance-Based Learning** or `Lazy Learning`. Instance-Based Learning is a type of machine learning where the model makes predictions for new instances by comparing them to the training instances based on a similarity measure.\n",
    "\n",
    "In Instance-Based Learning, the training data is stored without explicitly building a general model. When a new instance is encountered, the algorithm retrieves the most similar instances from the training set based on a similarity measure, such as Euclidean distance or cosine similarity. The predictions for the new instance are then derived from the labels or values associated with those similar instances. \n",
    "\n",
    "Common examples of Instance-Based Learning algorithms include k-Nearest Neighbors (k-NN) and Locally Weighted Regression (LWR). These algorithms rely on the notion that similar instances tend to have similar output values, enabling predictions based on the knowledge encoded in the training data.\n",
    "\n",
    "Instance-Based Learning algorithms have the advantage of being able to adapt to different problem domains and handle complex decision boundaries. They can handle non-linear relationships and do not require explicit assumptions about the underlying data distribution. However, they may have higher computational costs during prediction as they need to compare the new instance with all the stored instances in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d145ad-48ba-48d1-808f-f62aa0237b7a",
   "metadata": {},
   "source": [
    "### Question 12:\n",
    "What&#39;s the difference between a model parameter and a hyperparameter in a learning\n",
    "algorithm?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fd38c-aa01-411a-8262-2c5cccc9bf27",
   "metadata": {},
   "source": [
    "1. Model Parameters: Model parameters are the internal variables that are learned by the algorithm during the training process. These parameters are part of the model itself and define its behavior and functionality. They are typically optimized by adjusting their values to minimize a chosen objective function or loss function. Model parameters are specific to the chosen model architecture and represent the \"knowledge\" the model has acquired from the training data.\n",
    "\n",
    "\n",
    "2. Hyperparameters: Hyperparameters, on the other hand, are external configuration choices or settings that are not learned from the data but are set before the learning algorithm is applied. These parameters define the behavior of the learning algorithm itself, rather than the model. They control aspects such as the model's complexity, convergence criteria, regularization techniques, and learning rate.\n",
    "\n",
    "Hyperparameters need to be set based on prior knowledge, experience, or trial and error. They impact the learning algorithm's performance and generalization ability but are not learned during the training process. Different hyperparameter settings can lead to different model behaviors and performance.\n",
    "\n",
    "Optimizing hyperparameters often involves techniques like grid search, random search, or more advanced methods like Bayesian optimization or evolutionary algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18eb330-ae16-4314-ad00-35f7a6b5442c",
   "metadata": {},
   "source": [
    "### Question 13:\n",
    "What are the criteria that model-based learning algorithms look for? What is the most popular\n",
    "method they use to achieve success? What method do they use to make predictions?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245bf9bf-d321-45c2-8d73-7fadcf2b247f",
   "metadata": {},
   "source": [
    "Model-based learning algorithms typically look for patterns, relationships, or structures within the training data to build a predictive model. The criteria they aim to fulfill include:\n",
    "\n",
    "1. Generalization: Model-based algorithms strive to learn a model that generalizes well to unseen data. The model should capture the underlying patterns and relationships in the training data, allowing it to make accurate predictions on new, unseen instances.\n",
    "\n",
    "2. Simplicity: Model-based algorithms often seek simplicity in the learned model. They aim to find a model that balances complexity and interpretability, avoiding overfitting or excessive complexity that may lead to poor generalization on new data.\n",
    "\n",
    "The most popular method used by model-based learning algorithms to achieve success is to formulate and optimize an objective or loss function that quantifies the discrepancy between the model's predictions and the true values in the training data. By minimizing this loss function, the algorithm iteratively adjusts the model's parameters or structure to improve its performance.\n",
    "\n",
    "To make predictions, model-based learning algorithms utilize the learned model to map new input instances to corresponding outputs. The specific method used for making predictions depends on the nature of the problem. For example:\n",
    "\n",
    "- In regression tasks, the model-based algorithm applies the learned regression model to predict continuous numerical values.\n",
    "- In classification tasks, the model-based algorithm uses the learned classification model to assign new instances to predefined categories or classes.\n",
    "- In probabilistic modeling, the algorithm estimates the probability distribution over possible outcomes given the input data.\n",
    "\n",
    "The prediction process involves applying the learned model's equations, rules, or transformations to the input features of new instances to obtain the predicted outputs or probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b547795-198a-41a1-8981-e22739fb8676",
   "metadata": {},
   "source": [
    "### Question 14:\n",
    "Can you name four of the most important Machine Learning challenges?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0f4b20-4413-48c7-a58e-cb93a590707b",
   "metadata": {},
   "source": [
    "1. Overfitting and Underfitting: Overfitting occurs when a model performs well on the training data but fails to generalize to new, unseen data. Underfitting, on the other hand, happens when a model is too simplistic and fails to capture the underlying patterns in the data. Balancing the model's complexity and capacity to generalize is a critical challenge in machine learning.\n",
    "\n",
    "2. Data Quality and Quantity: Machine learning algorithms heavily rely on the quality and quantity of the training data. Insufficient or noisy data can lead to poor model performance. Gathering high-quality, representative data and dealing with missing values, outliers, or biases are essential challenges in ensuring reliable and accurate learning.\n",
    "\n",
    "3. Feature Engineering and Selection: The selection and engineering of relevant features from the raw input data significantly impact the performance of machine learning models. Choosing informative features, handling high-dimensional data, and finding effective representations require domain expertise and careful exploration.\n",
    "\n",
    "4. Interpretability and Explainability: Many machine learning models, such as deep neural networks, operate as complex black boxes, making it challenging to interpret their decision-making process. Explainable AI is an emerging field that aims to develop models and techniques that can provide transparent and interpretable insights, enabling humans to understand and trust the decisions made by machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ce5e9-05ce-4770-897c-8a2ae5145763",
   "metadata": {},
   "source": [
    "### Question 15:\n",
    "What happens if the model performs well on the training data but fails to generalize the results\n",
    "to new situations? Can you think of three different options?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081c623-428b-4a02-b5f9-258e7d2a35b1",
   "metadata": {},
   "source": [
    "When a model performs well on the training data but fails to generalize to new situations, it indicates an issue of overfitting, where the model has become too specialized in capturing the training data's idiosyncrasies. Here are three possible options to address this problem:\n",
    "\n",
    "1. Regularization: Regularization techniques can be applied to prevent overfitting and improve generalization. One common approach is to introduce regularization terms in the model's objective function, such as L1 or L2 regularization. This adds a penalty for large parameter values, encouraging the model to favor simpler solutions and reducing over-reliance on training data nuances.\n",
    "\n",
    "2. Cross-Validation: Cross-validation is a technique that assesses the model's performance on multiple subsets of the data. It involves splitting the data into training and validation sets, training the model on the training set, and evaluating its performance on the validation set. By repeatedly performing this process with different splits, cross-validation provides a more robust estimate of the model's generalization performance and can help identify overfitting.\n",
    "\n",
    "3. Increase Data Size: Insufficient training data can contribute to overfitting. Increasing the dataset's size by collecting more data or applying data augmentation techniques can help the model capture a broader range of patterns and generalize better. With more diverse and representative data, the model becomes less prone to memorizing specific examples and can learn more robust representations.\n",
    "\n",
    "These options aim to address overfitting by promoting simpler models, assessing generalization performance, and increasing the data's diversity. It's important to strike a balance between model complexity and generalization capability to achieve optimal performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af3332a-14aa-484e-ba72-9947bb45d50d",
   "metadata": {},
   "source": [
    "### Question 16:\n",
    "What exactly is a test set, and why would you need one?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b947eea-70e3-456e-8b7c-db49961dfabc",
   "metadata": {},
   "source": [
    "A test set, also known as a validation set, is a portion of labeled data that is held out from the training process and used to assess the performance and generalization of a trained machine learning model. The test set serves as an independent evaluation of how well the model can make predictions on unseen data.\n",
    "\n",
    "The primary purpose of a test set is to estimate the model's performance in real-world scenarios, providing an unbiased evaluation of its predictive capabilities. By evaluating the model on data it has not seen during training, the test set helps determine how well the model can generalize and make accurate predictions on new, unseen instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5788c-9bea-4ae3-89ae-5dacb160cee5",
   "metadata": {},
   "source": [
    "### Question 17:\n",
    "What is a validation set&#39;s purpose?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de2515-3bcd-4d62-be8d-f6226b8acb62",
   "metadata": {},
   "source": [
    "The purpose of a validation set, also known as a development set or holdout set, is to fine-tune and optimize the performance of a machine learning model during the training process. It is a subset of the labeled data that is distinct from both the training set and the test set.\n",
    "\n",
    "The key purposes of a validation set include:\n",
    "\n",
    "1. Hyperparameter Tuning: The validation set is used to iteratively adjust the model's hyperparameters. Hyperparameters are configuration choices that affect the learning algorithm's behavior but are not learned from the data. By training the model with different hyperparameter settings and evaluating its performance on the validation set, you can select the hyperparameters that yield the best results. This process helps optimize the model's performance and generalization ability.\n",
    "\n",
    "2. Model Selection: The validation set is crucial for comparing and selecting the best model among different candidate models or architectures. By training multiple models with varying characteristics and evaluating their performance on the validation set, you can choose the model that exhibits the highest performance and generalization capability.\n",
    "\n",
    "3. Early Stopping: The validation set is used to monitor the model's training progress and detect early signs of overfitting. By periodically evaluating the model's performance on the validation set during the training process, you can identify the point where the model starts to overfit the training data. This allows you to stop training at an optimal iteration to prevent overfitting and achieve a model that generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66256069-6046-4745-b932-62bba61f6c2f",
   "metadata": {},
   "source": [
    "### Question 18:\n",
    "What precisely is the train-dev kit, when will you need it, how do you put it to use?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c40d2e-16d4-45c0-90da-354b06f23baf",
   "metadata": {},
   "source": [
    "### Question 19:\n",
    "What could go wrong if you use the test set to tune hyperparameters?\n",
    "\n",
    "**<span style='color:blue'>Answer</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2e069-334f-456a-8de6-522c6babd422",
   "metadata": {},
   "source": [
    "\n",
    "1. Overfitting to the test set: Tuning hyperparameters based on the test set can lead to overfitting, where the model becomes specifically tailored to perform well on the test set but fails to generalize to new, unseen data. The test set no longer provides an unbiased evaluation of the model's performance, compromising its ability to make accurate predictions on real-world data.\n",
    "\n",
    "2. Leakage of information: When using the test set for hyperparameter tuning, the test set becomes indirectly part of the training process. This can inadvertently introduce information from the test set into the model, leading to inflated performance estimates. Consequently, the model may not perform as well on truly unseen data.\n",
    "\n",
    "3. Invalidating statistical significance: The test set's purpose is to provide a final evaluation of the model's performance. By using the test set for hyperparameter tuning, the statistical significance of the evaluation is compromised. The performance metrics calculated on the test set are no longer reliable indicators of the model's true generalization ability.\n",
    "\n",
    "To mitigate these issues, it is crucial to keep the test set separate and untouched until the final evaluation stage. Hyperparameter tuning should be performed using a separate validation set or through cross-validation techniques that ensure an unbiased assessment of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c11ef9e-1604-4f63-9396-2ad69b12b66b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
